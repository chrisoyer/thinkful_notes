{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment:\n",
    "Now that you've learned about random forests and decision trees let's do an exercise in accuracy. You know that random forests are basically a collection of decision trees. But how do the accuracies of the two models compare?\n",
    "\n",
    "So here's what you should do. Pick a dataset. It could be one you've worked with before or it could be a new one. Then build the best decision tree you can.\n",
    "\n",
    "Now try to match that with the simplest random forest you can. For our purposes measure simplicity with runtime. Compare that to the runtime of the decision tree. This is imperfect but just go with it.\n",
    "\n",
    "Hopefully out of this you'll see the power of random forests, but also their potential costs. Remember, in the real world you won't necessarily be dealing with thousands of rows. It could be millions, billions, or even more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from sklearn import neighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import tree\n",
    "from IPython.display import Image  # A convenience for displaying visualizations.\n",
    "import pydotplus  # Package for rendering our tree.\n",
    "import graphviz  # Package for rendering our tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try random forest on ramen rating dataset that linear regression and KNN didn't suceed with\n",
    "# %load ../20/20.5_knn_challenge_drill.py\n",
    "#!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numpy': '1.16.4',\n",
       " 'pandas': '0.25.0',\n",
       " 'matplotlib': '3.1.1',\n",
       " 'seaborn': '0.9.0',\n",
       " 'sklearn': '0.0',\n",
       " 'IPython': '7.6.1',\n",
       " 'pydotplus': '2.0.2',\n",
       " 'graphviz': '0.11.1'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load ../../utility/version_print.py\n",
    "#record module versions used in cell 1\n",
    "#\n",
    "def version_recorder():\n",
    "    '''\n",
    "    only works if import is first cell run. prints and then returns dictionary with modules:version.\n",
    "    '''\n",
    "    import pkg_resources\n",
    "    resources = In[1].splitlines()\n",
    "    ##ADD: drop lines if not _from_ or _import_\n",
    "    version_dict = { resource.split()[1].split(\".\")[0] : pkg_resources.get_distribution(resource.split()[1].split(\".\")[0]).version for resource in resources }\n",
    "    return version_dict\n",
    "version_recorder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsquare is 0.5745301123670111\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data_sets/ramen-ratings.csv\", )\n",
    "#only include if brand is commonly tested\n",
    "brand_list = (df.Brand.value_counts() > 10).index\n",
    "df = df[df[\"Brand\"].isin(brand_list)]\n",
    " \n",
    "#top ten would be separate target\n",
    "df.drop(columns=\"Top Ten\", inplace=True)\n",
    "\n",
    "df.dropna(how='any', inplace=True)\n",
    "df = df[df.Stars.str.isdigit()]\n",
    "\n",
    "y = df.Stars\n",
    "y = pd.to_numeric(y)\n",
    "X = pd.get_dummies(df[[\"Style\", \"Country\", \"Brand\"]], drop_first=True)\n",
    "\n",
    "regression_model = LinearRegression()\n",
    "regression_model.fit(X, y)\n",
    "print(\"rsquare is {}\".format(regression_model.score(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsquare is 0.5106085635454151\n",
      "[ 0.01804503  0.14490214  0.12029419  0.0828164  -0.27015293]\n"
     ]
    }
   ],
   "source": [
    "cross_val_score(regression_model, X, y, cv=5)\n",
    "# Very poor CV. will try another Regression model.\n",
    "ridge_model = Ridge()\n",
    "ridge_model.fit(X, y)\n",
    "print(\"rsquare is {}\".format(ridge_model.score(X, y)))\n",
    "print(cross_val_score(ridge_model, X, y, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsquared is 0.461693812021445\n",
      "[-0.78304577 -0.21570234  0.01037006 -0.01967306 -0.58637207]\n"
     ]
    }
   ],
   "source": [
    "knn_model = neighbors.KNeighborsRegressor(n_neighbors=2)\n",
    "knn_model.fit(X, y)\n",
    "print(\"rsquared is {}\".format(knn_model.score(X, y)))\n",
    "print(cross_val_score(knn_model, X, y, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsquared is 0.6625044221755905\n",
      "[-0.18861091  0.01602315  0.12318203  0.02487342 -0.47738882]\n"
     ]
    }
   ],
   "source": [
    "knn_model2 = neighbors.KNeighborsRegressor(n_neighbors=10, weights='distance')\n",
    "knn_model2.fit(X, y)\n",
    "print(\"rsquared is {}\".format(knn_model2.score(X, y)))\n",
    "print(cross_val_score(knn_model2, X, y, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree score is 0.46698564593301434\n",
      "cross val scores are [0.41064639 0.44656489 0.38846154 0.40384615], avg 0.4123797414091438\n"
     ]
    }
   ],
   "source": [
    "tree_model_d5 = tree.DecisionTreeClassifier(criterion='entropy',\n",
    "                                        max_depth=5)\n",
    "tree_model_d5.fit(X, y)\n",
    "print(f\"tree score is {tree_model_d5.score(X, y)}\")\n",
    "cv_d5 = cross_val_score(tree_model_d5, X, y, cv=4)\n",
    "print(\"cross val scores are {}, avg {}\".format(cv_d5, cv_d5.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree score is 0.5473684210526316\n",
      "cross val scores are [0.39923954 0.45801527 0.38846154 0.36923077], avg 0.403736779648529\n"
     ]
    }
   ],
   "source": [
    "tree_model_g = tree.DecisionTreeClassifier(criterion='gini',\n",
    "                                        max_depth=10)\n",
    "tree_model_g.fit(X, y)\n",
    "print(f\"tree score is {tree_model_g.score(X, y)}\")\n",
    "cv_g = cross_val_score(tree_model_g, X, y, cv=4)\n",
    "print(\"cross val scores are {}, avg {}\".format(cv_g, cv_g.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree score is 0.5358851674641149\n",
      "cross val scores are [0.40684411 0.39312977 0.40384615 0.36538462], avg 0.3923011616717535\n"
     ]
    }
   ],
   "source": [
    "tree_model = tree.DecisionTreeClassifier(criterion='entropy',\n",
    "                                        max_depth=10)\n",
    "tree_model.fit(X, y)\n",
    "print(f\"tree score is {tree_model.score(X, y)}\")\n",
    "cv = cross_val_score(tree_model, X, y, cv=4)\n",
    "print(\"cross val scores are {}, avg {}\".format(cv, cv.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quite a bit of variation, but much better than the other models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest score is 0.707177033492823\n",
      "cross val scores are [0.39543726 0.40076336 0.32307692 0.32692308], avg 0.3615501552840101\n"
     ]
    }
   ],
   "source": [
    "rfc_mod = RandomForestClassifier(n_estimators=100,\n",
    "                                criterion='entropy',\n",
    "                                )\n",
    "rfc_mod.fit(X, y)\n",
    "print(f\"forest score is {rfc_mod.score(X, y)}\")\n",
    "cv_rfc = cross_val_score(rfc_mod, X, y, cv=4)\n",
    "print(\"cross val scores are {}, avg {}\".format(cv_rfc, cv_rfc.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest score is 0.707177033492823\n",
      "cross val scores are [0.42585551 0.38931298 0.30769231 0.31153846], avg 0.35859981490949766\n"
     ]
    }
   ],
   "source": [
    "rfc_mod_gini = RandomForestClassifier(n_estimators=100,\n",
    "                                criterion='gini',\n",
    "                                )\n",
    "rfc_mod_gini.fit(X, y)\n",
    "print(f\"forest score is {rfc_mod_gini.score(X, y)}\")\n",
    "cv_rfc_gini = cross_val_score(rfc_mod_gini, X, y, cv=4)\n",
    "print(\"cross val scores are {}, avg {}\".format(cv_rfc_gini, cv_rfc_gini.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try random search on forest hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 50, num = 5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples for a node split\n",
    "min_samples_split = [2, 5, None]\n",
    "# Minimum samples at each leaf\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Create the random grid\n",
    "random_grid = {'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "rfc_instance = RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object '<class 'sklearn.ensemble.forest.RandomForestClassifier'>' (type <class 'abc.ABCMeta'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5f282083f42b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrfc_random\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrfc_instance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrfc_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[0mbase_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     58\u001b[0m                             \u001b[1;34m\"it does not seem to be a scikit-learn estimator \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                             \u001b[1;34m\"as it does not implement a 'get_params' methods.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                             % (repr(estimator), type(estimator)))\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mnew_object_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot clone object '<class 'sklearn.ensemble.forest.RandomForestClassifier'>' (type <class 'abc.ABCMeta'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods."
     ]
    }
   ],
   "source": [
    "rfc_random = RandomizedSearchCV(estimator=rfc_instance, n_iter=100, cv=4, param_distributions=random_grid)\n",
    "rfc_random.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"best random forest parameters are: {rfc_random.best_params_}\")\n",
    "print(f\"best estimator is {rfc_random.best_estimator_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
