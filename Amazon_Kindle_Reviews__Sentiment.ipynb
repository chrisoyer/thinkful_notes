{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon_Kindle_Reviews__Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisoyer/thinkful_notes/blob/master/Amazon_Kindle_Reviews__Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24-DjJmVMsUk",
        "colab_type": "text"
      },
      "source": [
        "# Description:\n",
        "Goal: Amazon Reviews Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haKf5PoUM5dO",
        "colab_type": "text"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K2oSl1VM4ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#environment\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds8rh3VeGIao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "os.environ[\"SPARK_CLASSPATH\"] = '/content/spark-2.4.5-bin-hadoop2.7'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGmM9NimNDY2",
        "colab_type": "code",
        "outputId": "c73e23fd-b312-44a2-fb25-39ee24597a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "# Install spark-related depdencies for Python\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8MB 59kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 48.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=73d62cb4a1940c75edd84d08ef6b2171b317078c45e73bb80b7c3b36a0357cb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4y1rP_DIa4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master('local[*]') \\\n",
        "    .appName('Spark NLP') \\\n",
        "    .config(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:2.2.2\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_5yArpRPTg9",
        "colab_type": "code",
        "outputId": "44441b12-2933-4b9a-a003-b8cc7fe97952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV3EdpQix1Tt",
        "colab_type": "code",
        "outputId": "022590b9-8ffb-45e5-af4e-fcaef27969d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import urllib\n",
        "import pandas as pd\n",
        "from pyspark import SparkContext\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "!pip install spark-nlp\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from sparknlp.base import LightPipeline\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import RegexRule\n",
        "from sparknlp.base import DocumentAssembler, Finisher"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spark-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/0b/ebb2aa778090574a63a14f3da4446bc3424b3728cef0500e288e13f75c17/spark_nlp-2.4.3-py2.py3-none-any.whl (108kB)\n",
            "\r\u001b[K     |███                             | 10kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 20kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 30kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 40kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 51kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 61kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 71kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 81kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 92kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 102kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 7.2MB/s \n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kZfyUIEP1-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########\n",
        "#I am using a local copy of the above file, stored on gDrive, \n",
        "#instead of re-downloading the source file.\n",
        "###########\n",
        "source_url = r\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Kindle_Store_5.json.gz\"\n",
        "data_folder = r'/content/gdrive/My Drive/thinkful/colab_datasets/amazon_reviews/'\n",
        "reviews_arx = os.path.join(data_folder, 'reviews_Kindle_Store_5.json.gz')\n",
        "reviews_raw = os.path.join(data_folder, 'Grocery_and_Gourmet_Food_5.json')\n",
        "if not os.path.exists(reviews_raw):\n",
        "    if not os.path.exists(data_folder):\n",
        "        os.mkdir(data_folder)\n",
        "    if not os.path.exists(reviews_arx):\n",
        "        urllib.request.urlretrieve(source_url, filename=reviews_arx)\n",
        "    import shutil\n",
        "    import gzip\n",
        "    with gzip.open(reviews_arx, 'rb') as f_in:\n",
        "        with open(reviews_raw, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "SPARK_URL = \"local[*]\"\n",
        "APP_NAME  = \"amazon_food_reviews\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HrmTKPaPhWn",
        "colab_type": "code",
        "outputId": "e83b150c-aa81-482e-f2e4-098b66d74f8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(\"Spark NLP version\")\n",
        "sparknlp.version()\n",
        "print(\"Apache Spark version\")\n",
        "spark.version"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP version\n",
            "2.2.2\n",
            "Apache Spark version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.4.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_GDMh39QKnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_df = spark.read.options(inferschema = \"true\").json(reviews_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJGgdyXHppbz",
        "colab_type": "code",
        "outputId": "ef825f8e-e6b0-4c79-eb10-3d69bb6bb9fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "reviews_df.printSchema()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- asin: string (nullable = true)\n",
            " |-- helpful: array (nullable = true)\n",
            " |    |-- element: long (containsNull = true)\n",
            " |-- overall: double (nullable = true)\n",
            " |-- reviewText: string (nullable = true)\n",
            " |-- reviewTime: string (nullable = true)\n",
            " |-- reviewerID: string (nullable = true)\n",
            " |-- reviewerName: string (nullable = true)\n",
            " |-- summary: string (nullable = true)\n",
            " |-- unixReviewTime: long (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHRBLfUppuAq",
        "colab_type": "code",
        "outputId": "396aaf72-6422-44af-c3f0-1689171ded6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "reviews_df.show(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+------------------+--------------+\n",
            "|      asin|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|           summary|unixReviewTime|\n",
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+------------------+--------------+\n",
            "|B000F83SZQ| [0, 0]|    5.0|I enjoy vintage b...| 05 5, 2014|A1F6404F1VG29J|          Avidreader|Nice vintage story|    1399248000|\n",
            "|B000F83SZQ| [2, 2]|    4.0|This book is a re...| 01 6, 2014| AN0N05A9LIJEQ|            critters|      Different...|    1388966400|\n",
            "|B000F83SZQ| [2, 2]|    4.0|This was a fairly...| 04 4, 2014| A795DMNCJILA6|                 dot|             Oldie|    1396569600|\n",
            "|B000F83SZQ| [1, 1]|    5.0|I'd never read an...|02 19, 2014|A1FV0SX13TWVXQ|Elaine H. Turley ...|I really liked it.|    1392768000|\n",
            "|B000F83SZQ| [0, 1]|    4.0|If you like perio...|03 19, 2014|A3SPTOKDG7WBLN|  Father Dowling Fan|    Period Mystery|    1395187200|\n",
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+------------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meXz731U18UB",
        "colab_type": "code",
        "outputId": "af26d155-ab79-4d4c-978c-95b49f68af19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "reviews_df.select('overall').describe().show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------------+\n",
            "|summary|           overall|\n",
            "+-------+------------------+\n",
            "|  count|            982619|\n",
            "|   mean| 4.347801131466011|\n",
            "| stddev|0.9550557821749456|\n",
            "|    min|               1.0|\n",
            "|    max|               5.0|\n",
            "+-------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA260z_c3fT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_df = reviews_df.withColumn('sentiment_label_fr_score',\n",
        "                                   F.when(reviews_df[\"overall\"] >= 4, 'Positive')\n",
        "                                   .otherwise('Negative'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crs2GVvP5INN",
        "colab_type": "code",
        "outputId": "738e0975-755a-451f-ced8-cb1c3f126840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "reviews_df.show(10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+------------------------+\n",
            "|      asin|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|sentiment_label_fr_score|\n",
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+------------------------+\n",
            "|B000F83SZQ| [0, 0]|    5.0|I enjoy vintage b...| 05 5, 2014|A1F6404F1VG29J|          Avidreader|  Nice vintage story|    1399248000|                Positive|\n",
            "|B000F83SZQ| [2, 2]|    4.0|This book is a re...| 01 6, 2014| AN0N05A9LIJEQ|            critters|        Different...|    1388966400|                Positive|\n",
            "|B000F83SZQ| [2, 2]|    4.0|This was a fairly...| 04 4, 2014| A795DMNCJILA6|                 dot|               Oldie|    1396569600|                Positive|\n",
            "|B000F83SZQ| [1, 1]|    5.0|I'd never read an...|02 19, 2014|A1FV0SX13TWVXQ|Elaine H. Turley ...|  I really liked it.|    1392768000|                Positive|\n",
            "|B000F83SZQ| [0, 1]|    4.0|If you like perio...|03 19, 2014|A3SPTOKDG7WBLN|  Father Dowling Fan|      Period Mystery|    1395187200|                Positive|\n",
            "|B000F83SZQ| [0, 0]|    4.0|A beautiful in-de...|05 26, 2014|A1RK2OCZDSGC6R|    ubavka seirovska|              Review|    1401062400|                Positive|\n",
            "|B000F83SZQ| [0, 0]|    4.0|I enjoyed this on...|06 10, 2014|A2HSAKHC3IBRE6|            Wolfmist|Nice old fashione...|    1402358400|                Positive|\n",
            "|B000F83SZQ| [1, 1]|    4.0|Never heard of Am...|03 22, 2014|A3DE6XGZ2EPADS|                 WPY|Enjoyable reading...|    1395446400|                Positive|\n",
            "|B000FA64PA| [0, 0]|    5.0|Darth Maul workin...|10 11, 2013|A1UG4Q4D3OAH3A|                 dsa|          Darth Maul|    1381449600|                Positive|\n",
            "|B000FA64PA| [0, 0]|    4.0|This is a short s...|02 13, 2011| AQZH7YTWQPOBE|            Enjolras|Not bad, not exce...|    1297555200|                Positive|\n",
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdRjRT7iA3RD",
        "colab_type": "code",
        "outputId": "b52e402e-e26a-4d4a-9a09-f62823cec179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "reviews_df.dropna(how='any')\n",
        "reviews_df.show(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+------------------+--------------+------------------------+\n",
            "|      asin|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|           summary|unixReviewTime|sentiment_label_fr_score|\n",
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+------------------+--------------+------------------------+\n",
            "|B000F83SZQ| [0, 0]|    5.0|I enjoy vintage b...| 05 5, 2014|A1F6404F1VG29J|          Avidreader|Nice vintage story|    1399248000|                Positive|\n",
            "|B000F83SZQ| [2, 2]|    4.0|This book is a re...| 01 6, 2014| AN0N05A9LIJEQ|            critters|      Different...|    1388966400|                Positive|\n",
            "|B000F83SZQ| [2, 2]|    4.0|This was a fairly...| 04 4, 2014| A795DMNCJILA6|                 dot|             Oldie|    1396569600|                Positive|\n",
            "|B000F83SZQ| [1, 1]|    5.0|I'd never read an...|02 19, 2014|A1FV0SX13TWVXQ|Elaine H. Turley ...|I really liked it.|    1392768000|                Positive|\n",
            "|B000F83SZQ| [0, 1]|    4.0|If you like perio...|03 19, 2014|A3SPTOKDG7WBLN|  Father Dowling Fan|    Period Mystery|    1395187200|                Positive|\n",
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+------------------+--------------+------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3US6tcgkD4NS",
        "colab_type": "text"
      },
      "source": [
        "#### Add ID\n",
        "Add unique ID so reviews can be grouped again after exploding to do sentiment analysis by sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4p1KZJED3HN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_df = reviews_df.withColumn(\"unique_id\", F.monotonically_increasing_id())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32hkHSki_dUy",
        "colab_type": "text"
      },
      "source": [
        "# Spark NLP Sentiment Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKZA-kX_U7Fy",
        "colab_type": "text"
      },
      "source": [
        "Using example from jonsnow sparknlp\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvB1XubdqrL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use document assemble which puts data in annotaed form\n",
        "document_assembler = DocumentAssembler() \\\n",
        "                      .setInputCol(\"reviewText\") \\\n",
        "                      .setOutputCol(\"review_document\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z-2qOuiA1fn",
        "colab_type": "code",
        "outputId": "d861a94c-937a-4112-d8e6-92ba5f2b6631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "assembled = document_assembler.transform(reviews_df)\n",
        "assembled.select('review_document').take(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(review_document=[Row(annotatorType='document', begin=0, end=293, result=\"I enjoy vintage books and movies so I enjoyed reading this book.  The plot was unusual.  Don't think killing someone in self-defense but leaving the scene and the body without notifying the police or hitting someone in the jaw to knock them out would wash today.Still it was a good read for me.\", metadata={'sentence': '0'}, embeddings=[], sentence_embeddings=[])]),\n",
              " Row(review_document=[Row(annotatorType='document', begin=0, end=454, result=\"This book is a reissue of an old one; the author was born in 1910. It's of the era of, say, Nero Wolfe. The introduction was quite interesting, explaining who the author was and why he's been forgotten; I'd never heard of him.The language is a little dated at times, like calling a gun a &#34;heater.&#34;  I also made good use of my Fire's dictionary to look up words like &#34;deshabille&#34; and &#34;Canarsie.&#34; Still, it was well worth a look-see.\", metadata={'sentence': '0'}, embeddings=[], sentence_embeddings=[])]),\n",
              " Row(review_document=[Row(annotatorType='document', begin=0, end=374, result=\"This was a fairly interesting read.  It had old- style terminology.I was glad to get  to read a story that doesn't have coarse, crasslanguage.  I read for fun and relaxation......I like the free ebooksbecause I can check out a writer and decide if they are intriguing,innovative, and have enough of the command of Englishthat they can convey the story without crude language.\", metadata={'sentence': '0'}, embeddings=[], sentence_embeddings=[])]),\n",
              " Row(review_document=[Row(annotatorType='document', begin=0, end=100, result=\"I'd never read any of the Amy Brewster mysteries until this one..  So I am really hooked on them now.\", metadata={'sentence': '0'}, embeddings=[], sentence_embeddings=[])]),\n",
              " Row(review_document=[Row(annotatorType='document', begin=0, end=129, result='If you like period pieces - clothing, lingo, you will enjoy this mystery.  Author had me guessing at least 2/3 of the way through.', metadata={'sentence': '0'}, embeddings=[], sentence_embeddings=[])])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4RcNPjopw-a",
        "colab_type": "code",
        "outputId": "042f65b7-89a2-454d-9d4c-62fdf009f612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "#detect sentences\n",
        "sentence_finder = SentenceDetector() \\\n",
        "    .setExplodeSentences(False) \\\n",
        "    .setInputCols(\"review_document\") \\\n",
        "    .setOutputCol(\"sentence\") \n",
        "sentence_data = sentence_finder.transform(assembled)\n",
        "sentence_data.select(\"sentence\").limit(5).show(truncate=False)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|sentence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[[document, 0, 63, I enjoy vintage books and movies so I enjoyed reading this book., [sentence -> 0], [], []], [document, 66, 86, The plot was unusual., [sentence -> 1], [], []], [document, 89, 293, Don't think killing someone in self-defense but leaving the scene and the body without notifying the police or hitting someone in the jaw to knock them out would wash today.Still it was a good read for me., [sentence -> 2], [], []]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "|[[document, 0, 36, This book is a reissue of an old one;, [sentence -> 0], [], []], [document, 38, 65, the author was born in 1910., [sentence -> 1], [], []], [document, 67, 102, It's of the era of, say, Nero Wolfe., [sentence -> 2], [], []], [document, 104, 201, The introduction was quite interesting, explaining who the author was and why he's been forgotten;, [sentence -> 3], [], []], [document, 203, 292, I'd never heard of him.The language is a little dated at times, like calling a gun a &#34;, [sentence -> 4], [], []], [document, 293, 299, heater., [sentence -> 5], [], []], [document, 300, 304, &#34;, [sentence -> 6], [], []], [document, 307, 378, I also made good use of my Fire's dictionary to look up words like &#34;, [sentence -> 7], [], []], [document, 379, 393, deshabille&#34;, [sentence -> 8], [], []], [document, 395, 403, and &#34;, [sentence -> 9], [], []], [document, 404, 412, Canarsie., [sentence -> 10], [], []], [document, 413, 417, &#34;, [sentence -> 11], [], []], [document, 419, 454, Still, it was well worth a look-see., [sentence -> 12], [], []]]|\n",
            "|[[document, 0, 34, This was a fairly interesting read., [sentence -> 0], [], []], [document, 37, 141, It had old- style terminology.I was glad to get  to read a story that doesn't have coarse, crasslanguage., [sentence -> 1], [], []], [document, 144, 173, I read for fun and relaxation., [sentence -> 2], [], []], [document, 174, 174, ., [sentence -> 3], [], []], [document, 175, 175, ., [sentence -> 4], [], []], [document, 176, 176, ., [sentence -> 5], [], []], [document, 177, 177, ., [sentence -> 6], [], []], [document, 178, 178, ., [sentence -> 7], [], []], [document, 179, 374, I like the free ebooksbecause I can check out a writer and decide if they are intriguing,innovative, and have enough of the command of Englishthat they can convey the story without crude language., [sentence -> 8], [], []]]                                                                                                                                                                                                                                                                                 |\n",
            "|[[document, 0, 63, I'd never read any of the Amy Brewster mysteries until this one., [sentence -> 0], [], []], [document, 64, 64, ., [sentence -> 1], [], []], [document, 67, 100, So I am really hooked on them now., [sentence -> 2], [], []]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "|[[document, 0, 72, If you like period pieces - clothing, lingo, you will enjoy this mystery., [sentence -> 0], [], []], [document, 75, 129, Author had me guessing at least 2/3 of the way through., [sentence -> 1], [], []]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhC29_VIICcl",
        "colab_type": "code",
        "outputId": "aafb147e-7000-4e09-fb01-b4446b3302ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "first_obs = sentence_data.select('sentence') \\\n",
        "      .limit(1)\n",
        "first_obs_df = first_obs.select('sentence', F.explode(first_obs.sentence).alias('_sentence'))\n",
        "\n",
        "pd.set_option('max_colwidth', 150)\n",
        "first_obs_df.toPandas()[['_sentence']]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(document, 0, 63, I enjoy vintage books and movies so I enjoyed reading this book., {'sentence': '0'}, [], [])</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(document, 66, 86, The plot was unusual., {'sentence': '1'}, [], [])</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(document, 89, 293, Don't think killing someone in self-defense but leaving the scene and the body without notifying the police or hitting someone...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                               _sentence\n",
              "0                                         (document, 0, 63, I enjoy vintage books and movies so I enjoyed reading this book., {'sentence': '0'}, [], [])\n",
              "1                                                                                   (document, 66, 86, The plot was unusual., {'sentence': '1'}, [], [])\n",
              "2  (document, 89, 293, Don't think killing someone in self-defense but leaving the scene and the body without notifying the police or hitting someone..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj7MvOyW_zbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('max_colwidth', 50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uczEzYxQJVJ1",
        "colab_type": "code",
        "outputId": "1bd7eebf-728b-462c-ec51-61611b797f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "#Tokenize\n",
        "tokenizer = Tokenizer() \\\n",
        "              .setInputCols(['sentence']) \\\n",
        "              .setOutputCol('token')\n",
        "token_data = tokenizer.fit(sentence_data).transform(sentence_data)\n",
        "token_data.show(5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+------------------+--------------+------------------------+---------+--------------------+--------------------+--------------------+\n",
            "|      asin|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|           summary|unixReviewTime|sentiment_label_fr_score|unique_id|     review_document|            sentence|               token|\n",
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+------------------+--------------+------------------------+---------+--------------------+--------------------+--------------------+\n",
            "|B000F83SZQ| [0, 0]|    5.0|I enjoy vintage b...| 05 5, 2014|A1F6404F1VG29J|          Avidreader|Nice vintage story|    1399248000|                Positive|        0|[[document, 0, 29...|[[document, 0, 63...|[[token, 0, 0, I,...|\n",
            "|B000F83SZQ| [2, 2]|    4.0|This book is a re...| 01 6, 2014| AN0N05A9LIJEQ|            critters|      Different...|    1388966400|                Positive|        1|[[document, 0, 45...|[[document, 0, 36...|[[token, 0, 3, Th...|\n",
            "|B000F83SZQ| [2, 2]|    4.0|This was a fairly...| 04 4, 2014| A795DMNCJILA6|                 dot|             Oldie|    1396569600|                Positive|        2|[[document, 0, 37...|[[document, 0, 34...|[[token, 0, 3, Th...|\n",
            "|B000F83SZQ| [1, 1]|    5.0|I'd never read an...|02 19, 2014|A1FV0SX13TWVXQ|Elaine H. Turley ...|I really liked it.|    1392768000|                Positive|        3|[[document, 0, 10...|[[document, 0, 63...|[[token, 0, 2, I'...|\n",
            "|B000F83SZQ| [0, 1]|    4.0|If you like perio...|03 19, 2014|A3SPTOKDG7WBLN|  Father Dowling Fan|    Period Mystery|    1395187200|                Positive|        4|[[document, 0, 12...|[[document, 0, 72...|[[token, 0, 1, If...|\n",
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+------------------+--------------+------------------------+---------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crYElFna-P15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# needed only if json file incorrectly untarred\n",
        "#token_data.where(token_data['_corrupt_record'].isNotNull())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzGEAhHnN0ua",
        "colab_type": "code",
        "outputId": "0f4ffa40-614b-4c9d-ef21-b45ab878a03c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        }
      },
      "source": [
        "#Normalize\n",
        "normalizer = (Normalizer()\n",
        "                .setInputCols([\"token\"])\n",
        "                .setOutputCol('normed_token')\n",
        "               )\n",
        "normalizer_data = normalizer.fit(token_data).transform(token_data)\n",
        "pd.DataFrame(normalizer_data.take(5))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B000F83SZQ</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>5.0</td>\n",
              "      <td>I enjoy vintage books and movies so I enjoyed ...</td>\n",
              "      <td>05 5, 2014</td>\n",
              "      <td>A1F6404F1VG29J</td>\n",
              "      <td>Avidreader</td>\n",
              "      <td>Nice vintage story</td>\n",
              "      <td>1399248000</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0</td>\n",
              "      <td>[(document, 0, 293, I enjoy vintage books and ...</td>\n",
              "      <td>[(document, 0, 63, I enjoy vintage books and m...</td>\n",
              "      <td>[(token, 0, 0, I, {'sentence': '0'}, [], []), ...</td>\n",
              "      <td>[(token, 0, 0, I, {'sentence': '0'}, [], []), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B000F83SZQ</td>\n",
              "      <td>[2, 2]</td>\n",
              "      <td>4.0</td>\n",
              "      <td>This book is a reissue of an old one; the auth...</td>\n",
              "      <td>01 6, 2014</td>\n",
              "      <td>AN0N05A9LIJEQ</td>\n",
              "      <td>critters</td>\n",
              "      <td>Different...</td>\n",
              "      <td>1388966400</td>\n",
              "      <td>Positive</td>\n",
              "      <td>1</td>\n",
              "      <td>[(document, 0, 454, This book is a reissue of ...</td>\n",
              "      <td>[(document, 0, 36, This book is a reissue of a...</td>\n",
              "      <td>[(token, 0, 3, This, {'sentence': '0'}, [], []...</td>\n",
              "      <td>[(token, 0, 3, This, {'sentence': '0'}, [], []...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>B000F83SZQ</td>\n",
              "      <td>[2, 2]</td>\n",
              "      <td>4.0</td>\n",
              "      <td>This was a fairly interesting read.  It had ol...</td>\n",
              "      <td>04 4, 2014</td>\n",
              "      <td>A795DMNCJILA6</td>\n",
              "      <td>dot</td>\n",
              "      <td>Oldie</td>\n",
              "      <td>1396569600</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "      <td>[(document, 0, 374, This was a fairly interest...</td>\n",
              "      <td>[(document, 0, 34, This was a fairly interesti...</td>\n",
              "      <td>[(token, 0, 3, This, {'sentence': '0'}, [], []...</td>\n",
              "      <td>[(token, 0, 3, This, {'sentence': '0'}, [], []...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B000F83SZQ</td>\n",
              "      <td>[1, 1]</td>\n",
              "      <td>5.0</td>\n",
              "      <td>I'd never read any of the Amy Brewster mysteri...</td>\n",
              "      <td>02 19, 2014</td>\n",
              "      <td>A1FV0SX13TWVXQ</td>\n",
              "      <td>Elaine H. Turley \"Montana Songbird\"</td>\n",
              "      <td>I really liked it.</td>\n",
              "      <td>1392768000</td>\n",
              "      <td>Positive</td>\n",
              "      <td>3</td>\n",
              "      <td>[(document, 0, 100, I'd never read any of the ...</td>\n",
              "      <td>[(document, 0, 63, I'd never read any of the A...</td>\n",
              "      <td>[(token, 0, 2, I'd, {'sentence': '0'}, [], [])...</td>\n",
              "      <td>[(token, 0, 1, Id, {'sentence': '0'}, [], []),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>B000F83SZQ</td>\n",
              "      <td>[0, 1]</td>\n",
              "      <td>4.0</td>\n",
              "      <td>If you like period pieces - clothing, lingo, y...</td>\n",
              "      <td>03 19, 2014</td>\n",
              "      <td>A3SPTOKDG7WBLN</td>\n",
              "      <td>Father Dowling Fan</td>\n",
              "      <td>Period Mystery</td>\n",
              "      <td>1395187200</td>\n",
              "      <td>Positive</td>\n",
              "      <td>4</td>\n",
              "      <td>[(document, 0, 129, If you like period pieces ...</td>\n",
              "      <td>[(document, 0, 72, If you like period pieces -...</td>\n",
              "      <td>[(token, 0, 1, If, {'sentence': '0'}, [], []),...</td>\n",
              "      <td>[(token, 0, 1, If, {'sentence': '0'}, [], []),...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0  ...                                                 14\n",
              "0  B000F83SZQ  ...  [(token, 0, 0, I, {'sentence': '0'}, [], []), ...\n",
              "1  B000F83SZQ  ...  [(token, 0, 3, This, {'sentence': '0'}, [], []...\n",
              "2  B000F83SZQ  ...  [(token, 0, 3, This, {'sentence': '0'}, [], []...\n",
              "3  B000F83SZQ  ...  [(token, 0, 1, Id, {'sentence': '0'}, [], []),...\n",
              "4  B000F83SZQ  ...  [(token, 0, 1, If, {'sentence': '0'}, [], []),...\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x84UyLOHOkoV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "53cb3fea-d14b-49d6-81b1-93380fc45e61"
      },
      "source": [
        "#Check Spelling\n",
        "\n",
        "# need to DL wordlist\n",
        "# -N new only\n",
        "# -P set directory\n",
        "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/spell/words.txt -P /tmp\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-11 01:26:46--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/spell/words.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.1.62\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.1.62|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4862966 (4.6M) [text/plain]\n",
            "Saving to: ‘/tmp/words.txt’\n",
            "\n",
            "words.txt           100%[===================>]   4.64M  7.05MB/s    in 0.7s    \n",
            "\n",
            "2020-03-11 01:26:47 (7.05 MB/s) - ‘/tmp/words.txt’ saved [4862966/4862966]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51sI2Oh1EPSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# spell check option 1\n",
        "spell_check_nv = (NorvigSweetingApproach()\n",
        "                .setInputCols(['normed_token'])\n",
        "                .setOutputCol('spell_checked')\n",
        "                .setDictionary(\"/tmp/words.txt\")\n",
        "                .setDoubleVariants(False)  # set for speed\n",
        "                .setShortCircuit(True)  # set for speed\n",
        "               )\n",
        "spell_check_data = spell_check_nv.fit(normalizer_data).transform(normalizer_data)\n",
        "spell_check_data.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBLK4Wr3IG6o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f0a7161-6301-452e-87f2-e4a1d73019f5"
      },
      "source": [
        "# spell check option 2\n",
        "sherlock_url = r\"http://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "! wget -O sherlockholmes.txt $sherlock_url -P /tmp\n",
        "train_corpus = (spark.read.text(\"./sherlockholmes.txt\")\n",
        "    .withColumnRenamed(\"value\", \"text\")\n",
        "                )\n",
        "\n",
        "spell_check_sym = (\n",
        "    SymmetricDeleteApproach()\n",
        "    .setInputCols([\"normed_token\"])\n",
        "    .setOutputCol(\"spell_checked\")\n",
        "    .setDictionary(\"/tmp/words.txt\")\n",
        "    .fit(train_corpus)\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-11 01:28:17--  http://www.gutenberg.org/files/1661/1661-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607788 (594K) [text/plain]\n",
            "Saving to: ‘sherlockholmes.txt’\n",
            "\n",
            "ockholmes.txt       100%[===================>] 593.54K   453KB/s    in 1.3s    \n",
            "\n",
            "2020-03-11 01:28:19 (453 KB/s) - ‘sherlockholmes.txt’ saved [607788/607788]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o264.fit.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`normed_token`' given input columns: [text];;\n'Project ['normed_token]\n+- Project [value#365 AS text#367]\n   +- Relation[value#365] text\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1358)\n\tat com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteApproach.validateDataSet(SymmetricDeleteApproach.scala:157)\n\tat com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteApproach.train(SymmetricDeleteApproach.scala:121)\n\tat com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteApproach.train(SymmetricDeleteApproach.scala:19)\n\tat com.johnsnowlabs.nlp.AnnotatorApproach.fit(AnnotatorApproach.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-8fe48bb744fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spell_checked\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msetDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/words.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`normed_token`' given input columns: [text];;\\n'Project ['normed_token]\\n+- Project [value#365 AS text#367]\\n   +- Relation[value#365] text\\n\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlh-BpEmPGpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sentiment\n",
        "sentiment_analyzer = (ViveknSentimentApproach()\n",
        "                      .setInputCols(['spell_checked', 'sentence'])\n",
        "                      .setOutputCol('sentiment')\n",
        "                      .setPruneCorpus(0)\n",
        "                      .setSentimentCol('sentiment_label')\n",
        "                      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFcDzo4RQstJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"sentiment\"]) \\\n",
        "    .setIncludeMetadata(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzqy7NcrQvr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_pipe = Pipeline(stages=[document_assembler,\n",
        "                                  sentence_finder,\n",
        "                                  tokenizer,\n",
        "                                  normalizer,\n",
        "                                  #spell_check_sym,  # option 2\n",
        "                                  sentiment_analyzer,\n",
        "                                  finisher\n",
        "                                  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKuHFMLFQ9nN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b402f53b-55cc-4bb9-e186-83c014fc9028"
      },
      "source": [
        "review_sentiment_model = (sentiment_pipe\n",
        "                          .fit(reviews_df)\n",
        "                          .transform(reviews_df)\n",
        "                          )"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o347.fit.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`sentiment_label`' given input columns: [overall, review_document, asin, helpful, token, reviewTime, unique_id, sentence, reviewerName, normed_token, sentiment_label_fr_score, summary, reviewText, unixReviewTime, reviewerID];;\n'Project [token#401, 'sentiment_label]\n+- Project [asin#6, helpful#7, overall#8, reviewText#9, reviewTime#10, reviewerID#11, reviewerName#12, summary#13, unixReviewTime#14L, sentiment_label_fr_score#115, unique_id#216L, review_document#372, sentence#386, token#401, UDF(array(token#401)) AS normed_token#417]\n   +- Project [asin#6, helpful#7, overall#8, reviewText#9, reviewTime#10, reviewerID#11, reviewerName#12, summary#13, unixReviewTime#14L, sentiment_label_fr_score#115, unique_id#216L, review_document#372, sentence#386, UDF(array(sentence#386)) AS token#401]\n      +- Project [asin#6, helpful#7, overall#8, reviewText#9, reviewTime#10, reviewerID#11, reviewerName#12, summary#13, unixReviewTime#14L, sentiment_label_fr_score#115, unique_id#216L, review_document#372, UDF(array(review_document#372)) AS sentence#386]\n         +- Project [asin#6, helpful#7, overall#8, reviewText#9, reviewTime#10, reviewerID#11, reviewerName#12, summary#13, unixReviewTime#14L, sentiment_label_fr_score#115, unique_id#216L, UDF(reviewText#9) AS review_document#372]\n            +- Project [asin#6, helpful#7, overall#8, reviewText#9, reviewTime#10, reviewerID#11, reviewerName#12, summary#13, unixReviewTime#14L, sentiment_label_fr_score#115, monotonically_increasing_id() AS unique_id#216L]\n               +- Project [asin#6, helpful#7, overall#8, reviewText#9, reviewTime#10, reviewerID#11, reviewerName#12, summary#13, unixReviewTime#14L, CASE WHEN (overall#8 >= cast(4 as double)) THEN Positive ELSE Negative END AS sentiment_label_fr_score#115]\n                  +- Relation[asin#6,helpful#7,overall#8,reviewText#9,reviewTime#10,reviewerID#11,reviewerName#12,summary#13,unixReviewTime#14L] json\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1358)\n\tat com.johnsnowlabs.nlp.annotators.sda.vivekn.ViveknSentimentApproach.train(ViveknSentimentApproach.scala:74)\n\tat com.johnsnowlabs.nlp.annotators.sda.vivekn.ViveknSentimentApproach.train(ViveknSentimentApproach.scala:14)\n\tat com.johnsnowlabs.nlp.AnnotatorApproach.fit(AnnotatorApproach.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-2f2bdd5d6df7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreview_sentiment_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`sentiment_label`' given input columns: [overall, review_document, asin, helpful, token, reviewTime, unique_id, sentence, reviewerName, normed_token, sentiment_label_fr_score, summary, reviewText, unixReviewTime, reviewerID];;\\n'Project [token#401, 'sentiment_label]\\n+- Project [asin#6, helpful#7, overall#8, reviewText#9, reviewTime#10, reviewerID#11, reviewerName#12, summary#13, unixReviewTime#14L, sentiment_label_fr_score#115, unique_id#216L, review_document#372, sentence#386, token#401, UDF(array(token#401)) AS normed_token#417]\\n   +- Project [asin#6, helpful#7, overall#8, reviewText#9, reviewTime#10, reviewerID#11, reviewerName#12, summary#13, unixReviewTime#14L, sentiment_label_fr_score#115, unique_id#216L, review_document#372, sentence#386, UDF(array(sentence#386)) AS token#401]\\n      +- Project [asin#6, helpful#7, overall#8, reviewText#9, reviewTime#10, reviewerID#11, reviewerName#12, summary#13, unixReviewTime#14L, sentiment_label_fr_score#115, unique_id#216L, review_document#372, UDF(array(review_document#372)) AS sentence#386]\\n         +- Project [asin#6, helpful#7, overall#8, reviewText#9, reviewTime#10, reviewerID#11, reviewerName#12, summary#13, unixReviewTime#14L, sentiment_label_fr_score#115, unique_id#216L, UDF(reviewText#9) AS review_document#372]\\n            +- Project [asin#6, helpful#7, overall#8, reviewText#9, reviewTime#10, reviewerID#11, reviewerName#12, summary#13, unixReviewTime#14L, sentiment_labe..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vonsa9lTtKro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_sentiment_data.show(n=5, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3Lzr1eaC95l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_sentiment_combined_df = (\n",
        "    review_sentiment_model\n",
        "    .groupBy(\"unique_id\", \"reviewerID\", \"unixReviewTime\")\n",
        "    .agg(avg(\"sentiment\", avg(\"overall\")))\n",
        ")\n",
        "review_sentiment_combined_df.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiogQ-pa9tv2",
        "colab_type": "text"
      },
      "source": [
        "# Using pretrained pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4B0pToj9oht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sparknlp.start()\n",
        "pipeline = PretrainedPipeline(name='analyze_sentiment', lang='en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzPKzbtdRjRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prepipe = PretrainedPipeline(name='analyze_sentiment')\n",
        "result = prepipe.annotate(target=reviews_df, column=\"reviewText\")\n",
        "result.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYXGNIY1xU7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark.createDataFrame(result.select('sentiment').take(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zaBuqMLZkeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result.select('sentiment').show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lmpJSsMuUFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get sentiment average for \n",
        "result.withColumn('avg_sentiment', \n",
        "                  F.when(F.col('sentiment')['result']==F.lit('positive'), F.lit((1,1))) \\\n",
        "                         .otherwise((0,1)) \\\n",
        "                         .reduce(lambda x, y: (x[0]+y[0], x[1]+y[1])) \\\n",
        "                         .mapValues(lambda x: x[0]/x[1])) \\\n",
        "      .show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGZx8xY9QgA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result.withColumn(\"sent_mean\", lambda x: x[\"sentiment\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG8ZPyXx4IWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result.withColumn(\"exploded_sent\", F.explode(F.col(\"sentiment\"))).select(\"exploded_sent\").printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw8b7kwfzYfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "schema = StructType([StructField])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wD3T0VRBZC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result2 = result \\\n",
        "  .withColumn(\"reviewID\", F.monotonically_increasing_id()) \\\n",
        "  .withColumn(\"exploded_sent\", F.explode(F.col(\"sentiment\"))) \\\n",
        "  .select([\"exploded_sent.*\", \"overall\", \"sentiment_label\", \"reviewID\"])\n",
        "result2.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8Glh7sjCg7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result2.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmvBMRWjC4t9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result2 = result2.withColumn(\"id\", F.monotonically_increasing_id())\n",
        "result2.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC6G3ljITPyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result3 = result2 \\\n",
        "  .withColumn(\"numerical_result\", F.when(result2[\"result\"] == \"Positive\", 1).otherwise(0)) \\\n",
        "  .groupBy([\"ID\", \"overall\", \"sentiment_label\"]) \\\n",
        "  .agg(F.mean(\"numerical_result\").alias(\"result_mean\")) \\\n",
        "  .show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZytky8n9vnr",
        "colab_type": "text"
      },
      "source": [
        "## Timeseries Analysis\n",
        "I am grouping the data by date to reduce the data size. This will get it to a manageable size for in-memory ml tools. I will then use statsmodels ARIMA to model the change in sentiment for reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRvUxh819vHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ts_df = ('review_sentiment_data'\n",
        "         .withColumn(rev_date_dt, col(\"unixReviewTime\").cast(\"DateType\")\n",
        "         .groupBy(\"rev_date_dt\", \"asin\")\n",
        "         .agg(avg(\"sentiment\", avg(\"overall\")))\n",
        "         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HMsxiz22UxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ts_pdf = ts_df.toPandas()\n",
        "ts_pdf.plot.line();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGUHjJnD1iuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFYIxiEJdOZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}