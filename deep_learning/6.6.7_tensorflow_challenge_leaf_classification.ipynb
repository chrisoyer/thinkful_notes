{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasource: \"Leafsnap: A Computer Vision System for Automatic Plant Species Identification,\"  \n",
    "Neeraj Kumar, Peter N. Belhumeur, Arijit Biswas, David W. Jacobs, W. John Kress, Ida C. Lopez, Jo√£o V. B. Soares,  \n",
    "Proceedings of the 12th European Conference on Computer Vision (ECCV),  \n",
    "October 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarurl = r'http://leafsnap.com/static/dataset/leafsnap-dataset.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Documents\\\\GitHub\\\\datasets\\\\dataset\\\\images\\\\field'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Documents\\GitHub\\datasets\n"
     ]
    }
   ],
   "source": [
    "%cd ../../datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O $tarurl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tar -xvf leafsnap-dataset.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "import skimage\n",
    "import os\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: './dataset/images/field/'\n",
      "C:\\Users\\User\\Documents\\GitHub\\datasets\\dataset\\images\\field\n"
     ]
    }
   ],
   "source": [
    "%cd ./dataset/images/field/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree_specs = os.popen('ls').read()\n",
    "tree_specs = tree_specs.strip().split(sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file test already exists.\n",
      "Error occurred while processing: test.\n",
      "A subdirectory or file train already exists.\n",
      "Error occurred while processing: train.\n",
      "A subdirectory or file vdate already exists.\n",
      "Error occurred while processing: vdate.\n"
     ]
    }
   ],
   "source": [
    "!mkdir test,train,vdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#move images into train, test, validate folders with subfolders for classes\n",
    "test_ratio = .15\n",
    "for spec in tree_specs:\n",
    "    %cd ./$spec\n",
    "    listing = os.popen('ls').read().strip().split(sep='\\n')\n",
    "    random.shuffle(listing) #inplace\n",
    "    im_count = len(listing)\n",
    "    test_size=val_size = math.ceil(test_ratio*im_count)\n",
    "    train_size = im_count - (test_size + val_size)\n",
    "    subfolders = ['train', 'test', 'vdate']\n",
    "    for subfolder in subfolders:\n",
    "        os.makedirs(os.path.join(os.path.dirname(os.getcwd()), subfolder, spec))\n",
    "    for item in range(0, train_size):\n",
    "        moved = listing.pop()\n",
    "        !mv $moved ../train/$spec/$moved\n",
    "    for item in range(0, test_size):\n",
    "        moved = listing.pop()\n",
    "        !mv $moved ../test/$spec/$moved\n",
    "    for item in range(0, val_size):\n",
    "        moved = listing.pop()\n",
    "        !mv $moved ../vdate/$spec/$moved\n",
    "    %cd ..\n",
    "    !rmdir ./$spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "train_augmented_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                             rotation_range=30,\n",
    "                                             zoom_range=0.15,\n",
    "                                             width_shift_range=0.2,\n",
    "                                             height_shift_range=0.2,\n",
    "                                             shear_range=0.15,\n",
    "                                             fill_mode='nearest',\n",
    "                                             horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "vdate_datagen = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1303 images belonging to 48 classes.\n"
     ]
    }
   ],
   "source": [
    "# Flow training images in batches of 128 using train_datagen generator\n",
    "batch_size = 32\n",
    "target_size = (256,256)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        r'./train/',  # This is the source directory for training images\n",
    "        target_size=target_size, \n",
    "        batch_size=batch_size,\n",
    "        color_mode='rgb',\n",
    "        # Specify the classes explicitly\n",
    "        classes = tree_specs,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1303 images belonging to 48 classes.\n"
     ]
    }
   ],
   "source": [
    "train_augmented_generator = train_augmented_datagen.flow_from_directory(\n",
    "        r'./train/',  # This is the source directory for training images\n",
    "        target_size=target_size, \n",
    "        batch_size=batch_size,\n",
    "        color_mode='rgb',\n",
    "        classes = tree_specs,\n",
    "        class_mode='categorical',\n",
    "        #augmentation\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 272 images belonging to 48 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        r'./test/',\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        color_mode='rgb',\n",
    "        classes=tree_specs,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 272 images belonging to 48 classes.\n"
     ]
    }
   ],
   "source": [
    "vdate_generator = vdate_datagen.flow_from_directory(\n",
    "        r'./vdate/',\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        color_mode='rgb',\n",
    "        classes=tree_specs,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 256, 256, 3)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of image array\n",
    "train_generator.next()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_generator.next()[0].shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_13 (Flatten)         (None, 196608)            0         \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 128)               25165952  \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 48)                3120      \n",
      "=================================================================\n",
      "Total params: 25,177,328\n",
      "Trainable params: 25,177,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequential_mod = tf.keras.models.Sequential()\n",
    "sequential_mod.add(Flatten(input_shape=input_shape))\n",
    "sequential_mod.add(Dense(128, activation='relu'))\n",
    "sequential_mod.add(Dropout(.1))\n",
    "sequential_mod.add(Dense(64, activation='relu'))\n",
    "sequential_mod.add(Dropout(.1))\n",
    "sequential_mod.add(Dense(48, activation='softmax'))\n",
    "\n",
    "sequential_mod.summary()\n",
    "\n",
    "sequential_mod.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sample = train_generator.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch: The shape of labels (received (1536,)) should equal the shape of logits except for the last dimension (received (32, 48)).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-3349a49028a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_sample\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         verbose=1)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[0;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[0;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[0;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[0;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[0;32m    312\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m               training=training))\n\u001b[0m\u001b[0;32m    253\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reduction'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m           \u001b[0mper_sample_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m           weighted_losses = losses_utils.compute_weighted_loss(\n\u001b[0;32m    168\u001b[0m               \u001b[0mper_sample_losses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    219\u001b[0m       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n\u001b[0;32m    220\u001b[0m           y_pred, y_true)\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(y_true, y_pred, from_logits, axis)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m   return K.sparse_categorical_crossentropy(\n\u001b[1;32m--> 978\u001b[1;33m       y_true, y_pred, from_logits=from_logits, axis=axis)\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m   4547\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4548\u001b[0m     res = nn.sparse_softmax_cross_entropy_with_logits_v2(\n\u001b[1;32m-> 4549\u001b[1;33m         labels=target, logits=output)\n\u001b[0m\u001b[0;32m   4550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4551\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mupdate_shape\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0moutput_rank\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits_v2\u001b[1;34m(labels, logits, name)\u001b[0m\n\u001b[0;32m   3475\u001b[0m   \"\"\"\n\u001b[0;32m   3476\u001b[0m   return sparse_softmax_cross_entropy_with_logits(\n\u001b[1;32m-> 3477\u001b[1;33m       labels=labels, logits=logits, name=name)\n\u001b[0m\u001b[0;32m   3478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[1;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[0;32m   3391\u001b[0m                        \u001b[1;34m\"should equal the shape of logits except for the last \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3392\u001b[0m                        \"dimension (received %s).\" % (labels_static_shape,\n\u001b[1;32m-> 3393\u001b[1;33m                                                      logits.get_shape()))\n\u001b[0m\u001b[0;32m   3394\u001b[0m     \u001b[1;31m# Check if no reshapes are required.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3395\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape mismatch: The shape of labels (received (1536,)) should equal the shape of logits except for the last dimension (received (32, 48))."
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "history = sequential_mod.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=int(total_sample/batch_size),  \n",
    "        epochs=n_epochs,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 254, 254, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 252, 252, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 126, 126, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 126, 126, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 508032)            0         \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 128)               65028224  \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 48)                6192      \n",
      "=================================================================\n",
      "Total params: 65,044,560\n",
      "Trainable params: 65,044,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_model = tf.keras.models.Sequential()\n",
    "conv_model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "conv_model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "conv_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "conv_model.add(Dropout(0.20))\n",
    "conv_model.add(Flatten())\n",
    "conv_model.add(Dense(128, activation='relu'))\n",
    "conv_model.add(Dropout(0.2))\n",
    "conv_model.add(Dense(48, activation='softmax'))\n",
    "\n",
    "conv_model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prevent error due to file errors\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 253s 6s/step - loss: 3.8166 - accuracy: 0.0370\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 256s 6s/step - loss: 3.7610 - accuracy: 0.0582\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 268s 7s/step - loss: 3.7265 - accuracy: 0.0724\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 259s 6s/step - loss: 3.6818 - accuracy: 0.0873\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 263s 7s/step - loss: 3.6486 - accuracy: 0.0905\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 283s 7s/step - loss: 3.6368 - accuracy: 0.0873\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 263s 7s/step - loss: 3.5736 - accuracy: 0.0999\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 241s 6s/step - loss: 3.5915 - accuracy: 0.0865\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 221s 6s/step - loss: 3.5251 - accuracy: 0.1094\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 207s 5s/step - loss: 3.5258 - accuracy: 0.1164\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "conv_history = conv_model.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=int(total_sample/batch_size),  \n",
    "        epochs=n_epochs,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [3.795672606896641,\n",
       "  3.757952099986943,\n",
       "  3.7581600013630863,\n",
       "  3.7287140352157606,\n",
       "  3.7049942029732406,\n",
       "  3.666092145733717,\n",
       "  3.64963339333643,\n",
       "  3.6518405988214524,\n",
       "  3.6174685336022185,\n",
       "  3.5936855615170145],\n",
       " 'accuracy': [0.048780486,\n",
       "  0.05743509,\n",
       "  0.059008654,\n",
       "  0.07631786,\n",
       "  0.0920535,\n",
       "  0.092840284,\n",
       "  0.08733281,\n",
       "  0.093627065,\n",
       "  0.101494886,\n",
       "  0.107789144]}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "40/40 [==============================] - 211s 5s/step - loss: 3.6638 - accuracy: 0.0716\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 220s 5s/step - loss: 3.6289 - accuracy: 0.0763\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 221s 6s/step - loss: 3.6136 - accuracy: 0.0844\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 223s 6s/step - loss: 3.6244 - accuracy: 0.0769\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 224s 6s/step - loss: 3.6423 - accuracy: 0.0582\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 221s 6s/step - loss: 3.5844 - accuracy: 0.0883\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 216s 5s/step - loss: 3.6123 - accuracy: 0.0832\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 218s 5s/step - loss: 3.5896 - accuracy: 0.0952\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 218s 5s/step - loss: 3.5783 - accuracy: 0.0779\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 219s 5s/step - loss: 3.5699 - accuracy: 0.0999\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 219s 5s/step - loss: 3.5741 - accuracy: 0.0818\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 218s 5s/step - loss: 3.5677 - accuracy: 0.0952\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 221s 6s/step - loss: 3.5463 - accuracy: 0.1047\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 219s 5s/step - loss: 3.5660 - accuracy: 0.0998\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 218s 5s/step - loss: 3.5534 - accuracy: 0.0952\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 219s 5s/step - loss: 3.5492 - accuracy: 0.0991\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 220s 5s/step - loss: 3.5537 - accuracy: 0.0889\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 219s 5s/step - loss: 3.5087 - accuracy: 0.1086\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 223s 6s/step - loss: 3.5304 - accuracy: 0.1063\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 219s 5s/step - loss: 3.5120 - accuracy: 0.1204\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "conv_aug_history = conv_model.fit_generator(\n",
    "        train_augmented_generator, \n",
    "        steps_per_epoch=int(total_sample/batch_size),  \n",
    "        epochs=n_epochs,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
