{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multivariate analysis vs univariate analysis, usually bivariate analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the variables features if we think they are well-suited to work within our model to explain the target variable. \n",
    "\n",
    "Put the target at the center: there should be one target variable unless model is very complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### variable transformation reasons:\n",
    "    \n",
    "1. Machine learning models can only work with numeric variables. So, categorical variables that have text values need to be converted to numeric values.  \n",
    "    a. one hot encoding: encoded to x-1 for the number of features. the listed variables are called dummy or indicator variables, the unnamed variable is the reference variable \n",
    "2. Some machine learning models assume the target variable to be normally distributed. In order to use these models, we may need to transform our target to be normally distributed.\n",
    "3. Some machine learning models are very sensitive to the relative magnitude of values. So, we may need to limit the values of the variables to some fixed range. Usually, we do this by normalizing our variables.\n",
    "4. To help our intuition and our understanding of the data, we may want to transform variables to a different unit of measurement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization** is the rescaling of a variable into the [0,1] range (including 0 and 1). For this purpose, we'll use SKLearn's `.normalize()` method from the  preprocessing module.  \n",
    "**Standardization** is the rescaling of a variable so its mean becomes 0 and its standard deviation becomes 1. Notice in the standardization we don't apply a maximum value for the variable. To apply standardization, we'll use SKLearn's `.scale()` method from the preprocessing module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature selection\n",
    "\n",
    "1. Filter:  \n",
    "    rank and eliminate below cut-off. Variance thresholds, correlation of feature with outcome are examples. generally 'cheap'  \n",
    "2. Wrapper methods:  \n",
    "    select sets. can be computationally demanding.  \n",
    "    forward passes - stepwise adding  \n",
    "    backward passes - stepwise removing  \n",
    "3. embedded methods:  \n",
    "    select sets of features, instrinsic to model. This may involve regularization, where a \"complexity penalty\" is added to the fitness measures typically used to assess the predictive power of a model. less intensive than wrapper.  \n",
    "4. Dimensionality reduction methods:  \n",
    "    especially PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variables can be\n",
    "1. **Continuous variables**:  \n",
    "    a. interval (lack absolute zero point)  \n",
    "    b. ratio variables  \n",
    "2. **Categorical** variables:  \n",
    "    a. ordinal variables (ordered, but no distance between them)  \n",
    "    b. nominal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use data types to find bad values:\n",
    "`# print all values that cannot be converted to float  \n",
    "for column_name in [\"Video Uploads\", \"Subscribers\"]:\n",
    "    print(\"These are the problematic values for the variable: {}\".format(column_name))\n",
    "    for value in youtube_df[\"Video Uploads\"]:\n",
    "        try:\n",
    "            float(value)\n",
    "        except:\n",
    "            print(value)`  \n",
    "            \n",
    "then fix  \n",
    "`\n",
    "youtube_df[\"Video Uploads\"] = youtube_df[\"Video Uploads\"].apply(str.strip).replace(\"--\", np.nan)\n",
    "youtube_df[\"Video Uploads\"] = pd.to_numeric(youtube_df[\"Video Uploads\"], downcast=\"float\")\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for categorical values, look at counts\n",
    "`youtube_df.Grade.value_counts()`\n",
    "will show unsual value (usually bad) or typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**outliers**: can  \n",
    "* drop  \n",
    "* cap eg winsorize  \n",
    "* transform (eg. monotogic transform like log)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing\n",
    "\n",
    "z = (x - u) / s\n",
    "\n",
    "standardized = (x - $\\bar{x}$) / std_dev  \n",
    "where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
