{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "* Shannon Entropy $H$, defined as: $ H = -\\sum_{i=1}^n P(x_i)log_2 P(x_i) $  \n",
    "    * decreases as number of possible outcomes decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to do:\n",
    "conda install -c anaconda graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "* Downsides:\n",
    "    * ramdomness in generation, which can lead to variance in estimates. Also not built same way each time.\n",
    "    * **they are incredibly prone to overfitting**, especially if  deep or complex. \n",
    "    * Because they work from info gain, they are biased towards the dominant class, so **balanced data is needed**.\n",
    "* ID3 algorithm\n",
    "    * requirements:\n",
    "        * binary outcome\n",
    "        * categorical features\n",
    "    * splits on the most valuable attribute (causes largest drop of H) in each feature\n",
    "* Random forests\n",
    "    * create many trees and use them to vote\n",
    "    * uses bagging to get better diversity\n",
    "    * uses random subspace - only uses some of the features on each tree\n",
    "        * generally for a dataset with x features $\\sqrt{x}$ features are used for classifiers and $x/3$ for regression.\n",
    "    * only works on test values that are within the training values\n",
    "    * black box algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVMs)\n",
    "* __S__upport __V__ector __C__lassifiers\n",
    "* builds hyperplane\n",
    "    * A hyperplane in n-dimensional space is an (n-1)-dimensional space.\n",
    "* soft margin in when can't classify observations on one side or the other\n",
    "    * SVC deals with this by cost function:\n",
    "        * maximize margin size vs \n",
    "        * minimize cumulative distance of points on wrong side from boundary\n",
    "* kernel trick \n",
    "    * unrelated to computer kernels\n",
    "    * kernels map data to space using weights. eg kernel smoothing.\n",
    "    * in SVM a kernel is a function that implicitly computes the dot product between two vectors in a higher-dimensional space without actually transforming the vectors into that space.\n",
    "    * additional reading:\n",
    "        * post [post by Eric Kim](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html), \n",
    "        * then this [paper by Martin Hoffman](http://www.cogsys.wiai.uni-bamberg.de/teaching/ss06/hs_svm/slides/SVM_Seminarbericht_Hofmann.pdf). \n",
    "        * This [hour lecture from Patrick Winston](https://www.youtube.com/watch?v=_PwhiWxHK8o) is a good short-breathed derivation of everything you've covered in SVM so far. \n",
    "        * If you don't have a good intuition of why dot products are important, check out this [video by Sal Khan](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/vector-dot-product-and-vector-length) from Khan Academy's linear algebra sequence.\n",
    "    * kernel choice\n",
    "        * default kernel - radial basis function, which uses a Gaussian decay according to the distance from the original point. \n",
    "        * other kernels: choose if [geometry justifies](https://stats.stackexchange.com/a/18032)\n",
    "        * can use cross validation in training set, but if tuning other hyperparameters, can be time consuming\n",
    "* SVM extensions\n",
    "    * simplest: hold-on-out on each classification (class vs all others), get confidence estimate for each, and classification with highest estimate (based on distance to boundary and classifier accuracy) wins\n",
    "    * pairwise boundaries; classification based on highest 1-vs-1 wins count\n",
    "* __S__upport __V__ector __R__egression\n",
    "    * works in reverse: penalty based on distance from far points\n",
    "    * parameters\n",
    "        * C: box constraint and sets the penalty for being outside of our margin. \n",
    "        * Epsilon: sets the size of our margin. \n",
    "    * advantage: can set sensitivity before model creation, not just after\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meshgrid example\n",
    "Used for SVC or KNN classifier  \n",
    "Example for 22.2\n",
    "```\n",
    "# Visualize our model\n",
    "y_min, y_max = X.test.min() - 1, X.test.max() + 3\n",
    "x_min, x_max = X.project.min() - 1, X.project.max() + 3\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .1),\n",
    "                     np.arange(y_min, y_max, .1))\n",
    "\n",
    "Z = (svm.predict(np.c_[xx.ravel(), yy.ravel()])=='pass')\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(test_data.project[0:10], test_data.test[0:10], marker='x')\n",
    "plt.scatter(test_data.project[10:20], test_data.test[10:20], marker='o')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xlabel('Project Grade')\n",
    "plt.ylabel('Test Grade')\n",
    "plt.title('Passing Grades SVM Example')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "* very popular for use\n",
    "* boosting models vary by:\n",
    "    * Type of simple model; almost any work.\n",
    "    * Index of error. You can use residuals from regression, classification errors, or any cost function.\n",
    "    * How the next iteration targets the error. You can weight inaccurately-predicted cases high and accurately-predicted cases low, you can directly model residuals, or you can model only the subset of the data that was inaccurately predicted.\n",
    "    * Stopping rule. You can stop once you've run a certain number of models, once the amount of variance explained by the most recent iteration of the model is lower than some threshold, or once the change in weights between the two most recent model iterations is lower than some threshold.\n",
    "* Gradient boosting can work on any combination of loss function and model type, as long as we can calculate the derivatives of the loss function with respect to the model parameters. \n",
    "* for regression trees:\n",
    "    * minimize $\\frac1{n}\\sum_{i=1}^n(y_i-(\\alpha + \\beta x_i))^2$\n",
    "    * takes residual of previous weak learner tree and tries to predict that. loop.\n",
    "    * often better than random forests, and less prone to overfitting that individual trees.\n",
    "        * can also subsample, so each iteration only gets part of the data\n",
    "        * can use shrinkage (a la Ridge regr) to lower impact of subsequent submodels\n",
    "            * learning rate 0 means only final combined model matters,\n",
    "            * learning rate 1 - all weighted equally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble models\n",
    "\n",
    "   * made up of other models\n",
    "   * types:\n",
    "       * bagging aka \"boostrap aggregating\" -drawing with replacement\n",
    "       * boosting - serial models daisy-chained together\n",
    "       * multiple models as first step, then output of these used as input for final model. Combination of other two methods\n",
    "    * \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison  \n",
    "  \n",
    "### REGRESSION\n",
    "* Linear Regression\n",
    "    * Pro: understandable, updatable with stochastic gradient descent\n",
    "    * Con: poor capture of nonlinearity, must manually find interaction terms\n",
    "* Regression trees\n",
    "    * Pro: learn nonlinear relationships easily, high performance\n",
    "    * Con: overfitting without ensembles, poor understandibility\n",
    "\n",
    "* Deep learning\n",
    "    * Pro: state of art, wide application with image, audio, text, easily updated\n",
    "    * Con: requires lots of data, computationally intensive\n",
    "* Nearest Neighbors\n",
    "    * Pro: ?\n",
    "    * Con: memory intensive, poor performance on high dimension data\n",
    "\n",
    "### CLASSIFICATION\n",
    "* Logistic Regression\n",
    "    * Pro: outputs have good probabalistic interpretation, easily regularized, easy updat\n",
    "    * Con: difficulty with complex relationships\n",
    "* Classification trees\n",
    "    * Pro: robust to outliers, scalable, handle nonlinear decision boundaries\n",
    "* Deep learning: same\n",
    "* Support Vector Machines\n",
    "    * Pro: can model nonlinearity, many kernels to choose from, robust to overfitting\n",
    "    * Con: memory intensive, tricky to pick kernel\n",
    "* Naive Bayes:\n",
    "    * Pro: Simple, scalable\n",
    "    * Con: less performant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
